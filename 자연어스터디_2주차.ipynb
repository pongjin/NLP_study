{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **2주차 피드백**\n","***  \n","**임베딩에 대해 더 깊은 공부 필요**  \n","\n","1. ### **_빈도 기반 표현_**  \n","    1. n-gram\n","    2. BoW\n","    3. DTM \n","    4. TF-IDF\n","\n","2. ### **_Count / Tfidf vectorizer 주요 파라미터_**  \n","    1. stop_words\n","    2. max_features\n","    3. max / min_df\n","    4. ngram_range\n","    5. token pattern\n","\n","3. ### **_World Embedding (1)_**  \n","    1. W2V(Word2Vec)\n","        * CBOW\n","        * Skip-gram\n","    2. Keras Embedding()\n","    3. FastText\n","\n","\n","4. ### **_분류 모델 실습(파라미터 위주)_**  \n","    1. 임베딩 기반\n","    2. 빈도 기반\n"],"metadata":{"id":"CZwbGX7zNkoL"}},{"cell_type":"markdown","source":["## Library"],"metadata":{"id":"fUvnf9IPGmMb"}},{"cell_type":"code","source":["# base\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# regex\n","import re\n","\n","# stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# tokenizer\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import WordPunctTokenizer\n","from nltk.tokenize import TreebankWordTokenizer\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","# lemmatization\n","from nltk.stem import WordNetLemmatizer\n","\n","# Vectorizer\n","f\n","\n","# modeling\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split, cross_val_score\n","\n","#scoring\n","from sklearn.metrics import f1_score, accuracy_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGuElkaDGpPk","outputId":"6c73813c-3191-4c2a-85df-882f1fc7085c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrMYx1ibG2wN","outputId":"44d28eab-8a57-46a8-fb36-7ba6eaf20222"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## 공부\n","1. n-gram\n","2. 빈도 기반 단어표현\n","2. vectorizer 파라미터 \n","3. 워드 임베딩\n","4. 머신러닝 분류 모델 실습 ( tfidf, 임베딩)\n","5. 문서 유사도 (tfidf)"],"metadata":{"id":"s__l3vVGovnE"}},{"cell_type":"markdown","source":["# 1. **n-gram**\n","n그램은 빈도 기반 통계적 언어 모델로 특정 단어가 나올 확률을 **앞의** n개의 단어를 통해 계산하는 방법이다  \n","\n","EX ) i was a car  \n","n=1(unigram) : 'i' / 'was' / 'a' / 'car' $$p(car|a)$$ \n","n=2(bigram) : 'i was' / 'was a' / 'a car' $$p(car|was\\;a)$$ \n","n=3(trigram) : 'i was a' / 'was a car' $$p(car|i\\;was\\;a)$$ \n","n=4(4-gram) : 'i was a car'  \n","  \n","한계  \n","0. 특정 부분만 뽑는것은 문맥 고려가 힘드니 전체 문장을 고려한 모델보다 정확도가 떨어짐\n","1. 희소성 극복 X\n","2. trade-off : 대부분 n=2일때가 성능이 더 좋긴함. n이 커질수록 희소성도 커지며 모델 사이즈도 커짐. n이 작을수록 근사 정확도가 떨어짐 n<5 로 권장됨\n","\n","# 2. **빈도 기반 단어 표현**\n","![이미지](https://wikidocs.net/images/page/31767/wordrepresentation.PNG)  \n","로컬 표현은 단어의 의미, 뉘앙스를 표현 할 수 없다는 한계를 가짐.  \n","\n","## BoW, DTM, TF-IDF\n","**Bow**  \n","단어 순서를 고려하지 않는 빈도 기반 단어표현방법. 문서 내 단어들의 횟수를 수치화하여 문서 간 유사도나 분류 문제를 수행가능  \n","  \n","**DTM**   \n","문서들의 BoW를 결합한 문서 단어 행렬(Document-Term Matrix)  \n","한계\n","1. 희소성 극복 X\n","2. 단순 빈도 수 기반 접근(중요한 단어인지는 빈도수는 알 수 없음)  \n","**TF-IDF**   \n","TF : 특정 문서에서 특정 단어의 등장 빈도  \n","DF : 특정 단어를 포함한 문서의 개수  \n","IDF : DF의 역수  $$idf(d, t) = log(\\frac{n}{1+df(t)})$$\n","TF-IDF : TF * IDF 값으로 특정 문서에서 단어의 빈도에 가중치를 부여. 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰이며 해당 단어가 특정 문서에서 값이 높아도, 많은 문서에서 등장한다면 낮은 값을 가진다.\n","\n","# 3. **vectorizer params**\n","### CountVectorizer \n","단어 빈도화 시키는 함수. 그동안 했던거 한번에 할 수 있다! 자세한 함수는 아래에서\n","### TfidfVectorizer \n","tfidf화 시켜주는 함수. 기존 idf식의 분자에 +1, 로그항에+1 , if-idf에 L2정규화\n"],"metadata":{"id":"SsRewNZUtD8E"}},{"cell_type":"markdown","source":["### CountVectorizer & TfidfVectorizer 파라미터 정리"],"metadata":{"id":"SO83fe6z5VOc"}},{"cell_type":"code","source":["#1. stop_words : 불용어 처리 \n","text = [\"When I was 10, I was a car. it was very good!\"]\n","#직접지정\n","vect = CountVectorizer(stop_words=[\"I\"])\n","print('bag of words vector :',vect.fit_transform(text).toarray())\n","print('vocabulary :',vect.vocabulary_)\n","#내장\n","vect = CountVectorizer(stop_words=\"english\")\n","print('bag of words vector :',vect.fit_transform(text).toarray())\n","print('vocabulary :',vect.vocabulary_)\n","#NLTK불용어\n","stop_words = stopwords.words(\"english\")\n","vect = CountVectorizer(stop_words=stop_words)\n","print('bag of words vector :',vect.fit_transform(text).toarray())\n","print('vocabulary :',vect.vocabulary_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RH22aCfby_FF","outputId":"c61e0625-87a5-4d7d-ad86-5885779250bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bag of words vector : [[1 1 1 1 1 3 1]]\n","vocabulary : {'when': 6, 'was': 5, '10': 0, 'car': 1, 'it': 3, 'very': 4, 'good': 2}\n","bag of words vector : [[1 1 1]]\n","vocabulary : {'10': 0, 'car': 1, 'good': 2}\n","bag of words vector : [[1 1 1]]\n","vocabulary : {'10': 0, 'car': 1, 'good': 2}\n"]}]},{"cell_type":"code","source":["#lowercase : 소문자 처리 유무\n","# max / min_df : 최대 / 최소 빈도수 제한\n","# max_features : 높은 빈도 순으로 단어 개수 제한 \n","\n","# analyzer : 토큰화 단위. 단어 문자 등\n","vect = CountVectorizer(analyzer='char').fit(text)\n","print(vect.get_feature_names(),'\\n')\n","# tokenizer : 토큰화 함수\n","vect = CountVectorizer().fit(text)\n","print(vect.get_feature_names(),'\\n')\n","toks = WordPunctTokenizer()\n","vect = CountVectorizer(tokenizer=toks.tokenize).fit(text)\n","print(vect.get_feature_names(),'\\n')\n","# ngram_range : n그램 단위로 토큰화 (1,2) 1단어 2단어 동시에\n","# 2단어가 의미있는 경우 (good job, well done)\n","vect = CountVectorizer(ngram_range=(1,2)).fit(text)\n","print(vect.get_feature_names())"],"metadata":{"id":"Qo_OomT26Zbf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d386ecfe-0ed5-4e3b-dc3d-0377d707b9e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[' ', '!', ',', '.', '0', '1', 'a', 'c', 'd', 'e', 'g', 'h', 'i', 'n', 'o', 'r', 's', 't', 'v', 'w', 'y'] \n","\n","['10', 'car', 'good', 'it', 'very', 'was', 'when'] \n","\n","['!', ',', '.', '10', 'a', 'car', 'good', 'i', 'it', 'very', 'was', 'when'] \n","\n","['10', '10 was', 'car', 'car it', 'good', 'it', 'it was', 'very', 'very good', 'was', 'was 10', 'was car', 'was very', 'when', 'when was']\n"]}]},{"cell_type":"code","source":["#token_pattern : 토큰화 하는 정규표현식 패턴 default : (?u)\\b\\w\\w+\\b\n","vect = CountVectorizer(token_pattern=\"[^\\w]\").fit(text)\n","print(vect.get_feature_names(),'\\n\\n')\n","#숫자가 문자형이여도 제거해줌\n","vect = CountVectorizer().fit(text)\n","print(vect.get_feature_names(),'\\n')\n","vect = CountVectorizer(token_pattern=\"[^\\d\\W]+\\w\").fit(text)\n","print(vect.get_feature_names())"],"metadata":{"id":"6mBmuI_F6ZX4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"acbef606-b542-43ae-c231-406db9fe9600"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[' ', '!', ',', '.'] \n","\n","\n","['10', 'car', 'good', 'it', 'very', 'was', 'when'] \n","\n","['car', 'good', 'it', 'very', 'was', 'when']\n"]}]},{"cell_type":"code","source":["# fit_transform 은 압축행렬을 생성함. fit 하고 transform 따로 한거랑은 다른 결과값임!\n","text = ['When I was 10, I was a car. it was very good!',\n","        'hi my name is car']\n","vect = CountVectorizer()      \n","toks = vect.fit_transform(text) #압축 행렬 변환\n","print(toks.shape)\n","print(toks.toarray()) # 원행렬 행렬\n","print(list(dict(sorted(vect.vocabulary_.items())).keys())) #단어들"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_RJk8m-9mmn","outputId":"f6bcbcde-d58c-46ba-87ee-14657c98717d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 11)\n","[[1 1 1 0 0 1 0 0 1 3 1]\n"," [0 1 0 1 1 0 1 1 0 0 0]]\n","['10', 'car', 'good', 'hi', 'is', 'it', 'my', 'name', 'very', 'was', 'when']\n"]}]},{"cell_type":"code","source":["#tfidfvectorizer 는 countvectorizer와 동일한 파리미터를 가짐\n","# norm : l1이나 l2냐. default 는 l2"],"metadata":{"id":"kvp4YPUYNH1r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. **워드 임베딩**\n","희소표현 : 벡터 또는 행렬의 값이 대부분 0으로 표현되는 방법 (원핫인코딩)  \n","워드임베딩(밀집표현) : 희소 표현을 압축 - 단어 간 유사도를 파악할 수 있음 > 의미부여 > 단어의 의미를 여러 차원에다가 분산 표현  \n","<br/>  \n","\n","워드 임베딩 방법론\n","1. LSA\n","2. Word2Vec\n","3. FastText\n","4. Glove\n","5. keras Embedding()  \n","\n","## CBOW(Continuous BOW)\n","![image](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)  \n","예측(중심단어), window(주변단어)=2\n","<br/>  \n","\n","![image](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n","![image](https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG)\n","$$W_{m \\times v} \\ \\ \\ \\ W^{\"}_{v \\times m}$$ 은 전치가 아닌 임의의 행렬이며 CBOW는 이 두 행렬을 학습하여 임베딩하는 기법  \n","\n","<br/>  \n","\n","![image](https://wikidocs.net/images/page/22660/word2vec_renew_4.PNG)  \n","\n","## Skip-gram  \n","\n","![image](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)\n"],"metadata":{"id":"5VMtZUNK6Ze-"}},{"cell_type":"code","source":["#사전훈련 임베딩모델\n","import gensim\n","import urllib.request\n","\n","# 구글의 사전 훈련된 Word2Vec 모델을 로드.\n","word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin', binary=True)"],"metadata":{"id":"krRPl1zpvN0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(word2vec_model.vectors.shape)\n","print(word2vec_model.similarity('social', 'network'))\n","print(word2vec_model.similarity('virtual', 'reality'))\n","#print(word2vec_model['book'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpec3Mq0yCTd","outputId":"59182827-2a7a-4844-8931-ece49a512cce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3000000, 300)\n","0.30620542\n","0.31206152\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","#example data\n","sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n","y_train = [1, 0, 0, 1, 1, 0, 1]\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","vocab_size = len(tokenizer.word_index) + 1\n","X_encoded = tokenizer.texts_to_sequences(sentences)\n","max_len = max(len(l) for l in X_encoded)\n","X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n","y_train = np.array(y_train)"],"metadata":{"id":"-WY1uGI-E1To"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_matrix = np.zeros((vocab_size, 300))"],"metadata":{"id":"1nBZHmCyE3T_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_vector(word):\n","    if word in word2vec_model:\n","        return word2vec_model[word]\n","    else:\n","        return None"],"metadata":{"id":"8qeRfxw5FPSi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for word, index in tokenizer.word_index.items():\n","    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n","    vector_value = get_vector(word)\n","    if vector_value is not None:\n","        embedding_matrix[index] = vector_value"],"metadata":{"id":"F8VYyLPbFPPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n","\n","model = Sequential()\n","model.add(Input(shape=(max_len,), dtype='int32'))\n","e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n","model.add(e)\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","model.fit(X_train, y_train, epochs=10, verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqk9WRLgFPMR","outputId":"37f472ae-6849-415a-e692-ff78ed3eb011"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1/1 - 1s - loss: 0.7041 - acc: 0.5714 - 602ms/epoch - 602ms/step\n","Epoch 2/10\n","1/1 - 0s - loss: 0.6854 - acc: 0.7143 - 11ms/epoch - 11ms/step\n","Epoch 3/10\n","1/1 - 0s - loss: 0.6672 - acc: 0.7143 - 6ms/epoch - 6ms/step\n","Epoch 4/10\n","1/1 - 0s - loss: 0.6496 - acc: 0.7143 - 7ms/epoch - 7ms/step\n","Epoch 5/10\n","1/1 - 0s - loss: 0.6325 - acc: 0.8571 - 5ms/epoch - 5ms/step\n","Epoch 6/10\n","1/1 - 0s - loss: 0.6160 - acc: 0.8571 - 7ms/epoch - 7ms/step\n","Epoch 7/10\n","1/1 - 0s - loss: 0.6000 - acc: 0.8571 - 6ms/epoch - 6ms/step\n","Epoch 8/10\n","1/1 - 0s - loss: 0.5846 - acc: 1.0000 - 6ms/epoch - 6ms/step\n","Epoch 9/10\n","1/1 - 0s - loss: 0.5696 - acc: 1.0000 - 6ms/epoch - 6ms/step\n","Epoch 10/10\n","1/1 - 0s - loss: 0.5552 - acc: 1.0000 - 5ms/epoch - 5ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5869e80ac0>"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["## Glove(Global Vectors for Word Representation)  \n","빈도 + 예측기반 임베딩 기법으로 word2vec이랑 성능 비슷함  \n","한줄요약 : \"임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것\"  \n","$$dot\\ product(w_{i}\\ \\tilde{w_{k}}) \\approx\\ log\\ P(k\\ |\\ i) = log\\ P_{ik}$$  \n","어려움...  \n","<br/>\n","## FastText\n","word2Vec의 확장판으로, 단어 안 내부단어인 subwords룰 고려하여 학습  \n","n = 3인 경우 : <ap, app, ppl, ple, le>, <apple>  \n","n = 3 ~ 6인 경우 : \n","<ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>\n","apple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>  \n","<br/>\n","장점 \n","1. 모르는 단어를 쪼개서 아는 단어들로 유사도를 구할 수 있다. (birthday -> birth, day)\n","2. 오타가 나도 계산 가능 (appple -> apple과 거의 유사)  \n","\n","##keras Embedding()\n","인공신경망으로 임베딩 층 구현\n","절차 : 정수 인코딩 > 룩업 테이블 내 정수 인덱스의 임베딩 벡터 값 리턴 > 학습 > 최종 벡터 도출  \n","\n"],"metadata":{"id":"rTFCFZTZiXWp"}},{"cell_type":"code","source":["#keras embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n","y_train = [1, 0, 0, 1, 1, 0, 1]"],"metadata":{"id":"hkUQUlOUBJTf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1\n","print('단어 집합 :',vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgGK0VXIDBEx","outputId":"db61bb34-0efb-43c4-c860-d50a72c801f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합 : 16\n"]}]},{"cell_type":"code","source":["import numpy as np\n","X_encoded = tokenizer.texts_to_sequences(sentences)\n","X_train = pad_sequences(X_encoded, maxlen=4, padding='post')\n","y_train = np.array(y_train)\n","print('패딩 결과 :')\n","print(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ssEW3t7YDGoN","outputId":"188de0f4-bc5b-42e3-d9ea-b752f469ed25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["패딩 결과 :\n","[[ 1  2  3  4]\n"," [ 5  6  0  0]\n"," [ 7  8  0  0]\n"," [ 9 10  0  0]\n"," [11 12  0  0]\n"," [13  0  0  0]\n"," [14 15  0  0]]\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Flatten\n","\n","embedding_dim = 4\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim, input_length=4))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","model.fit(X_train, y_train, epochs=10, verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kthys5Y4DGdw","outputId":"d73cd005-4e03-41f2-ed91-ba75db2e43e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1/1 - 1s - loss: 0.7102 - acc: 0.1429 - 568ms/epoch - 568ms/step\n","Epoch 2/10\n","1/1 - 0s - loss: 0.7084 - acc: 0.1429 - 12ms/epoch - 12ms/step\n","Epoch 3/10\n","1/1 - 0s - loss: 0.7066 - acc: 0.2857 - 11ms/epoch - 11ms/step\n","Epoch 4/10\n","1/1 - 0s - loss: 0.7048 - acc: 0.2857 - 8ms/epoch - 8ms/step\n","Epoch 5/10\n","1/1 - 0s - loss: 0.7030 - acc: 0.2857 - 8ms/epoch - 8ms/step\n","Epoch 6/10\n","1/1 - 0s - loss: 0.7013 - acc: 0.2857 - 8ms/epoch - 8ms/step\n","Epoch 7/10\n","1/1 - 0s - loss: 0.6995 - acc: 0.2857 - 7ms/epoch - 7ms/step\n","Epoch 8/10\n","1/1 - 0s - loss: 0.6977 - acc: 0.4286 - 10ms/epoch - 10ms/step\n","Epoch 9/10\n","1/1 - 0s - loss: 0.6960 - acc: 0.4286 - 10ms/epoch - 10ms/step\n","Epoch 10/10\n","1/1 - 0s - loss: 0.6942 - acc: 0.4286 - 7ms/epoch - 7ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f58673f8df0>"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["## ELMo ( Embeddings from language model )\n","양방향 문자 임베딩(CNN 챕터에서 설명)"],"metadata":{"id":"XOHJnvHJDGKO"}},{"cell_type":"code","source":[],"metadata":{"id":"Z3v8KTG0Hh8e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. **분류 모델 실습**\n","### 1. 임베딩 기반 "],"metadata":{"id":"hzWG77bp6ZAK"}},{"cell_type":"markdown","source":["##  임베딩"],"metadata":{"id":"frRY0Ys7KT-u"}},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/MyDrive/nlp_data/practice.csv')\n","data.columns = ['index','text','author']\n","data = data[['text','author']]\n","data.author = data.author.astype('str')\n","\n","#전처리부터 해야됨\n","data.text = data.text.apply(lambda x : re.sub('o‐m','om',x))\n","data.text = data.text.apply(lambda x : re.sub('o-m','om',x))\n","data.text = data.text.apply(lambda x : re.sub('o‐d','od',x))\n","data.text = data.text.apply(lambda x : re.sub('o-d','od',x))\n","\n","data.text = data.text.apply(lambda x : re.sub(\"-{2,}\",' ',x))\n","data.text = data.text.apply(lambda x : re.sub(\"[^0-9a-zA-Z\\'\\- ]\",'',x))\n","\n","data.text = data.text.apply(lambda x : re.sub(\"US\", \"usa\" ,x)) #특정 단어만 소문자 처리\n","\n","data_text = data.text.apply(lambda x : text_to_word_sequence(x)) #대문자 처리도 다 해줌\n","data_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R0ZepOim6ZQX","outputId":"a0f2ca83-4379-4ec0-e41d-08dae220ed7f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        [he, was, almost, choking, there, was, so, muc...\n","1               [your, sister, asked, for, it, i, suppose]\n","2        [she, was, engaged, one, day, as, she, walked,...\n","3        [the, captain, was, in, the, porch, keeping, h...\n","4        [have, mercy, gentlemen, odin, flung, up, his,...\n","                               ...                        \n","54874    [is, that, you, mr, smith, odin, whispered, i,...\n","54875    [i, told, my, plan, to, the, captain, and, bet...\n","54876    [your, sincere, well, wisher, friend, and, sis...\n","54877        [then, you, wanted, me, to, lend, you, money]\n","54878    [it, certainly, had, not, occurred, to, me, be...\n","Name: text, Length: 54879, dtype: object"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["# 불용어 처리\n","#stop_words_list = stopwords.words('english')\n","#data_text = data_text.apply(lambda x : [i for i in x if i not in stop_words_list])\n","#data_text"],"metadata":{"id":"9bTj4RbjZC7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#결측행 확인\n","print('전체 문서의 수 :',len(data_text))\n","cnt=[]\n","for i,j in enumerate(data_text):\n","  if j == []:\n","    cnt.append(i)\n","len(cnt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLFLmj3vute7","outputId":"2e1859c0-80b7-4cde-a7ed-d85b551c2743"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["전체 문서의 수 : 54879\n"]},{"output_type":"execute_result","data":{"text/plain":["44"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["idx = [i for i in data_text.index if i not in cnt]"],"metadata":{"id":"nOBqayLMvI4P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_text = data_text[idx]"],"metadata":{"id":"66aNiIPMvJzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#표제어 추출 하는게 좋은가??\n","# https://aclanthology.org/W19-6203.pdf\n","# https://stats.stackexchange.com/questions/374209/pre-processing-lemmatizing-and-stemming-make-a-better-doc2vec\n","# https://stackoverflow.com/questions/23877375/word2vec-lemmatization-of-corpus-before-training\n","# 정해진건 없지만 영어에서는 안해도 된다고는 한다..\n","\n","#불용어 처리도 안해도 된다는 의견이 있음\n","# https://stackoverflow.com/questions/34721984/stopword-removing-when-using-the-word2vec\n","# https://datascience.stackexchange.com/questions/80227/should-i-keep-common-stop-words-when-preprocessing-for-word-embedding\n","# 하지만 텍스트 분류와 감성분석에서는 불용어가 주는 정보가 거의 없기 때문에 제거하는것이 더 좋다고 한다.\n","# https://www.kaggle.com/general/208662"],"metadata":{"id":"StD72KmsWFS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lemmatize with POS Tag\n","from nltk.corpus import wordnet\n","\n","def get_wordnet_pos(word):\n","  #특정 단어의 품사(ex. VBD)의 앞글자(V)를 대문자로 추출\n","    tag = nltk.pos_tag([word])[0][1][0].upper() \n","\n","  #품사 딕셔너리 생성\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","# 단어 표제어 추출\n","#lemmatizer = WordNetLemmatizer()\n","#data_text = data_text.apply(lambda x : [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in x])"],"metadata":{"id":"6BXtO-Lw6ZLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.  \n","window = 컨텍스트 윈도우 크기  \n","min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)  \n","workers = 학습을 위한 프로세스 수  \n","sg = 0은 CBOW, 1은 Skip-gram."],"metadata":{"id":"sKr4fZxxcBIW"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","\n","model = Word2Vec(sentences=data_text, size=300, window=5, min_count=2, workers=4, sg=0)"],"metadata":{"id":"oFjvLZVOa7_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.wv.most_similar(\"man\") #불용어 처리도 안했음"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBdVvQxya5Ir","outputId":"e7adebeb-3c8e-4e5d-d710-4b028caead9d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('gentleman', 0.7922993302345276),\n"," ('fellow', 0.7626957297325134),\n"," ('woman', 0.7523154020309448),\n"," ('person', 0.6834543943405151),\n"," ('officer', 0.676331102848053),\n"," ('soldier', 0.6753376722335815),\n"," ('creature', 0.6553103923797607),\n"," ('lad', 0.6538023948669434),\n"," ('fellah', 0.6221071481704712),\n"," ('stranger', 0.6079340577125549)]"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["model.save(\"word2vec.model\")\n","model = Word2Vec.load(\"word2vec.model\")"],"metadata":{"id":"nQeApRIdxu2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 표제어 추출 하고 sg=0일때\n","#[('fellah', 0.8633233308792114), 농민!\n","# ('woman', 0.8444805145263672),\n","# ('fellow', 0.7350084781646729),\n","# ('chap', 0.7068322896957397),\n","# ('men', 0.7026264667510986),\n","# ('fashion', 0.7018184065818787),\n","# ('gentleman', 0.6731172800064087),\n","# ('soldier', 0.6719125509262085),\n","# ('people', 0.6682370901107788),\n","# (\"man's\", 0.6663216352462769)]"],"metadata":{"id":"OSe8_Mz2jVbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 표제어 추출 하고 sg=1일때 \n","#[('fellah', 0.7122722864151001),\n","# (\"man's\", 0.6552585363388062),\n","# ('fellow', 0.6448282599449158),\n","# ('bailey', 0.638940691947937),\n","# ('orlick', 0.632754921913147),\n","# ('englishman', 0.6306297183036804),\n","# ('pawnbroker', 0.6290642023086548),\n","# ('vagabond', 0.6286340355873108),\n","# ('shipmate', 0.6274350881576538),\n","# ('rascal', 0.6273226737976074)]"],"metadata":{"id":"hB3MQoH_jH2r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#표제어 추출 안하고 sg=1일때 이렇게 나옴\n","#[('woman', 0.684684157371521),\n","# ('fellah', 0.672799825668335),\n","# ('officer', 0.6544522643089294),\n","# ('fellow', 0.6477473974227905),\n","# ('soldier', 0.6471898555755615),\n","# ('mans', 0.6460458040237427),\n","# (\"man's\", 0.6431757807731628),\n","# ('person', 0.6349670886993408),\n","# ('gentleman', 0.6260026693344116),\n","# ('gentlemans', 0.6248079538345337)]"],"metadata":{"id":"L1DvNZORiIih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#표제어 추출 안하고 sg=0일 때는 이렇게 나옴... 뭐가 더 좋은걸까??\n","#[('woman', 0.8838928937911987),\n","# ('fellah', 0.8600987195968628),\n","# ('fellow', 0.7722310423851013),\n","# ('mans', 0.7648662328720093),\n","# ('gentleman', 0.7474823594093323),\n","# ('asp', 0.721663236618042),\n","# ('men', 0.7075345516204834),\n","# ('creature', 0.7051717042922974),\n","# ('soldier', 0.705062985420227),\n","# ('chap', 0.6961818933486938)]"],"metadata":{"id":"k8_WDaqwecXi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_document_vectors(document_list):\n","    document_embedding_list = []\n","    nonidx=[]\n","\n","    # 각 문서에 대해서\n","    for idxs,line in enumerate(document_list):\n","        doc2vec = None\n","        count = 0\n","        for word in line:\n","            if word in model.wv.vocab:\n","                count += 1\n","                # 해당 문서에 있는 모든 단어들의 벡터값을 더한다.\n","                if doc2vec is None:\n","                    doc2vec = model[word]\n","                else:\n","                    doc2vec = doc2vec + model[word]\n","\n","        if doc2vec is not None:\n","            # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.\n","            doc2vec = doc2vec / count\n","            document_embedding_list.append(doc2vec)\n","        else:\n","          nonidx.append(idxs)\n","\n","    # 각 문서에 대한 문서 벡터 리스트를 리턴\n","    return document_embedding_list, nonidx"],"metadata":{"id":"9eE-3vw9mdTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["document_embedding_list,aa = get_document_vectors(data_text)"],"metadata":{"id":"cNzhSXMJm6Mx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(document_embedding_list),len(data_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-8KSIOonXh4","outputId":"1656cb18-1d08-4fd0-fca6-5c4cceb30cc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["54835 54835\n"]}]},{"cell_type":"code","source":["np.array(document_embedding_list).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kn8UyBnvMkUD","outputId":"c6a77051-3746-4bad-c165-04ccd1185eaf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(54835, 300)"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["y = data.author[idx].reset_index(drop=True)\n","X_train, X_test, y_train, y_test = train_test_split(np.array(document_embedding_list), y, test_size = 0.2, random_state = 156)"],"metadata":{"id":"VjaUM_dequ_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = LogisticRegression()\n","model.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uA2V4PAMp2mO","outputId":"9a300580-d331-4a3c-d0c9-455e2889edd1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression()"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["preds = model.predict(X_test)\n","preds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQk1ZxhP3_gj","outputId":"d5a7b9b3-e00f-4fba-c2be-d4c4016b29da"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['0', '0', '1', ..., '0', '0', '0'], dtype=object)"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["## 성능평가\n","1. 불용어 o 표제어 o : 0.53\n","2. 불용어 o 표제어 X : 0.51\n","3. 불용어 X 표제어 X : 0.53 size=100\n","4. 불용어 X 표제어 X : 0.62 size=300\n"],"metadata":{"id":"-TsCygM-8OdZ"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, accuracy_score\n","accuracy_score(preds, y_test) #53퍼센트"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbgGlu4B67uT","outputId":"c3e44c25-9176-4e72-845e-7f83d5fdee2b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.540713048235616"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["print(preds[:10],'\\n',list(y_test[:10])) #2를 잘 분류 못함"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9wz4lLH6-_V","outputId":"b78d3061-af66-4dc9-bfb7-50a2c54baa4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['0' '0' '1' '3' '3' '1' '3' '1' '2' '1'] \n"," ['2', '0', '0', '2', '3', '1', '3', '1', '1', '1']\n"]}]},{"cell_type":"code","source":["y_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7d4FjGojBd0d","outputId":"336418a7-33ff-406d-809b-b20fa9735fe2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18186    0\n","13356    1\n","13397    3\n","35662    4\n","28840    4\n","        ..\n","6955     1\n","7653     2\n","42402    1\n","39628    3\n","24108    2\n","Name: author, Length: 43868, dtype: object"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","source":["## 모델 파이프라인 구축\n","https://studying-modory.tistory.com/entry/210114-머신러닝-텍스트마이닝-두-번째-시간"],"metadata":{"id":"fDLiw8uYKCKA"}},{"cell_type":"markdown","source":["## 2. 빈도 기반 모델링"],"metadata":{"id":"K_hKZIHHP_Gt"}},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/MyDrive/nlp_data/practice.csv')\n","data.columns = ['index','text','author']\n","data = data[['text','author']]\n","\n","#전처리부터 해야됨\n","data.text = data.text.apply(lambda x : re.sub('o‐m','om',x))\n","data.text = data.text.apply(lambda x : re.sub('o-m','om',x))\n","data.text = data.text.apply(lambda x : re.sub('o‐d','od',x))\n","data.text = data.text.apply(lambda x : re.sub('o-d','od',x))\n","\n","data.text = data.text.apply(lambda x : re.sub(\"-{2,}\",' ',x))\n","data.text = data.text.apply(lambda x : re.sub(\"[^0-9a-zA-Z\\'\\- ]\",'',x))\n","\n","data.text = data.text.apply(lambda x : re.sub(\"US\", \"usa\" ,x)) #특정 단어만 소문자 처리"],"metadata":{"id":"YKqZ38blJ3CA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7Mku-zasduI","outputId":"ea12fe92-cfe4-4dbd-d726-9909244f1efb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        He was almost choking There was so much so muc...\n","1                       Your sister asked for it I suppose\n","2         She was engaged one day as she walked in peru...\n","3        The captain was in the porch keeping himself c...\n","4        Have mercy gentlemen odin flung up his hands D...\n","                               ...                        \n","54874    Is that you Mr Smith odin whispered I hardly d...\n","54875    I told my plan to the captain and between us w...\n","54876     Your sincere well-wisher friend and sister LU...\n","54877                 Then you wanted me to lend you money\n","54878    It certainly had not occurred to me before but...\n","Name: text, Length: 54879, dtype: object"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["tfidf_descp = TfidfVectorizer(max_features=20000, stop_words='english', min_df=2)\n","x_descp = tfidf_descp.fit_transform(data.text)"],"metadata":{"id":"vdSWanZTHCt8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tfidf_descp.get_feature_names_out()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4d2okphrGHFt","outputId":"12c73ee5-4d90-4199-82ef-512a40129416"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['10', '100', '1000', ..., 'zoology', 'zossimov', 'zphyrine'],\n","      dtype=object)"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["y = data.author"],"metadata":{"id":"gyswUPUxHRs8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" x_train, x_test, y_train, y_test = train_test_split(x_descp, y, test_size = 0.2, random_state = 156)"],"metadata":{"id":"I-Jt3QVnLy79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2 = LogisticRegression()"],"metadata":{"id":"-Jh_0q2sMU94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2.fit(x_train, y_train)\n","preds = model2.predict(x_test)"],"metadata":{"id":"zoU-ewSNL8D9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy_score(preds, y_test) #72퍼센트의 성능"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrXdC_wPN3OS","outputId":"c31f21d0-fa75-4717-a2cc-cbdedde339c0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7265852769679301"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["f1_score(preds, y_test, average='weighted')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FiVXESPnOAbK","outputId":"1f0432ce-18af-4937-aa41-475b12c0f309"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.72912247624235"]},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","source":["## max features가 얼만큼 영향을 미치는가?"],"metadata":{"id":"3DxAjm-Mlp9N"}},{"cell_type":"code","source":["max_feature = [100,500,1000,2500,5000,10000,20000,30000,50000,100000]\n","scores = []\n","for i in max_feature:\n","  tfidf_descp = TfidfVectorizer(max_features=i, stop_words='english', min_df=2)\n","  x_descp = tfidf_descp.fit_transform(data.text)\n","  x_train, x_test, y_train, y_test = train_test_split(x_descp, y, test_size = 0.2, random_state = 156)\n","  model2 = LogisticRegression()\n","  model2.fit(x_train, y_train)\n","  preds = model2.predict(x_test)\n","  scores.append(accuracy_score(preds, y_test))\n","scores # 2만이 제일 좋음"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90NufClKstkE","outputId":"9e6a8de7-ccef-42b1-dc44-17e385aad449"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.4518039358600583,\n"," 0.5603134110787172,\n"," 0.605502915451895,\n"," 0.6701895043731778,\n"," 0.7044460641399417,\n"," 0.7223943148688047,\n"," 0.7265852769679301,\n"," 0.7241253644314869,\n"," 0.7241253644314869,\n"," 0.7241253644314869]"]},"metadata":{},"execution_count":81}]},{"cell_type":"markdown","source":["## stopword빼고 ngram 넣어보자"],"metadata":{"id":"QFoIeNOWujz9"}},{"cell_type":"code","source":["ranges=[2,3,4]\n","features=[20000,40000]\n","scores = []\n","for i in ranges:\n","  for j in features:\n","    tfidf_descp = TfidfVectorizer(max_features=j, min_df=2, ngram_range=(1,i))\n","    x_descp = tfidf_descp.fit_transform(data.text)\n","    x_train, x_test, y_train, y_test = train_test_split(x_descp, y, test_size = 0.2, random_state = 156)\n","    model2 = LogisticRegression()\n","    model2.fit(x_train, y_train)\n","    preds = model2.predict(x_test)\n","    scores.append(accuracy_score(preds, y_test))\n","scores #2에 4만이 제일 좋긴함"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"reaELQzyt62p","outputId":"8cb441f5-2cb7-426e-b3b0-38b1ff7144a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.7584730320699709,\n"," 0.7652150145772595,\n"," 0.7532798833819242,\n"," 0.7628462099125365,\n"," 0.7533709912536443,\n"," 0.7629373177842566]"]},"metadata":{},"execution_count":86}]}]}