{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1_LCeXD_yxZ","executionInfo":{"status":"ok","timestamp":1674992700344,"user_tz":-540,"elapsed":17022,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"175351ff-8c3b-4e36-b9e6-026a73786730"},"id":"c1_LCeXD_yxZ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### 코랩에서 한글 fasttext 사전학습 모델로 임베딩을 하려면 이렇게 해야댐"],"metadata":{"id":"wXSe0TbjSzOE"},"id":"wXSe0TbjSzOE"},{"cell_type":"code","source":["! pip3 install --upgrade gensim"],"metadata":{"id":"aKRq91d9LOw-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674992717334,"user_tz":-540,"elapsed":13544,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"647429ef-7dbb-462f-f470-f7ac4b8def78"},"id":"aKRq91d9LOw-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n","Collecting gensim\n","  Downloading gensim-4.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting FuzzyTM>=0.4.0\n","  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n","Collecting pyfume\n","  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from FuzzyTM>=0.4.0->gensim) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n","Collecting simpful\n","  Downloading simpful-2.9.0-py3-none-any.whl (30 kB)\n","Collecting fst-pso\n","  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim) (1.15.0)\n","Collecting miniful\n","  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.25.1)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n","Building wheels for collected packages: fst-pso, miniful\n","  Building wheel for fst-pso (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20443 sha256=d7753137ee21b22e3630878c66e74f03daaa186a8211528d42ac811a40ec8f10\n","  Stored in directory: /root/.cache/pip/wheels/6a/65/c4/d27eeee9ba3fc150a0dae150519591103b9e0dbffde3ae77dc\n","  Building wheel for miniful (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3530 sha256=a3e5561b54e1e087da4e1238053142df3f9d6e22de1bb1628cbcf30a236d2607\n","  Stored in directory: /root/.cache/pip/wheels/ba/d9/a0/ddd93af16d5855dd9bad417623e70948fdac119d1d34fb17c8\n","Successfully built fst-pso miniful\n","Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM, gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 gensim-4.3.0 miniful-0.0.6 pyfume-0.2.25 simpful-2.9.0\n"]}]},{"cell_type":"code","source":["!pip uninstall numpy\n","!pip install numpy"],"metadata":{"id":"Xcn93tYCL3Yn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674992797313,"user_tz":-540,"elapsed":17258,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"ebb6c34a-34f3-4de6-ff5b-43b0cd7f11cd"},"id":"Xcn93tYCL3Yn","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.24.1\n","Uninstalling numpy-1.24.1:\n","  Would remove:\n","    /usr/local/bin/f2py\n","    /usr/local/bin/f2py3\n","    /usr/local/bin/f2py3.8\n","    /usr/local/lib/python3.8/dist-packages/numpy-1.24.1.dist-info/*\n","    /usr/local/lib/python3.8/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n","    /usr/local/lib/python3.8/dist-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so\n","    /usr/local/lib/python3.8/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n","    /usr/local/lib/python3.8/dist-packages/numpy/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled numpy-1.24.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting numpy\n","  Using cached numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","Installing collected packages: numpy\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.1 which is incompatible.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.24.1\n"]}]},{"cell_type":"code","source":["# 여기서 런타임 다시 시작 해야됨 !!!!!!!!!!!!!!!!!!!!!!!\n","import numpy as np\n","np.__version__ #1.24인지 확인"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"HJtG3ze6MEm5","executionInfo":{"status":"ok","timestamp":1674992877876,"user_tz":-540,"elapsed":6,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"27e07ee9-73f3-45d5-d17e-9aabd25439e9"},"id":"HJtG3ze6MEm5","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.24.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["#konlpy 설치\n","!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"],"metadata":{"id":"g9FPhNMeykRq"},"id":"g9FPhNMeykRq","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"d5ee67e9","metadata":{"id":"d5ee67e9"},"outputs":[],"source":["from konlpy.tag import Mecab, Okt"]},{"cell_type":"code","execution_count":null,"id":"8c785278","metadata":{"id":"8c785278"},"outputs":[],"source":["mecab = Mecab()\n","okt = Okt()"]},{"cell_type":"code","execution_count":null,"id":"d2c31e66","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"d2c31e66","executionInfo":{"status":"ok","timestamp":1674993038157,"user_tz":-540,"elapsed":7,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"989ddd3a-264e-4ec2-e3b4-796fa33d5031"},"outputs":[{"output_type":"stream","name":"stdout","text":["['드디어', '설치', '완료']\n"]}],"source":["words = '드디어 설치 완료'\n","print(mecab.morphs(words))"]},{"cell_type":"markdown","source":["[사전학습모델 다운로드](https://drive.google.com/file/d/1SNkg0POPYOgm2gRw-0uZmg3xQgOTw4eG/view?usp=share_link)"],"metadata":{"id":"vuIqUCD8XqrK"},"id":"vuIqUCD8XqrK"},{"cell_type":"code","source":["from gensim.models.fasttext import load_facebook_model\n","ft_model = load_facebook_model('/content/drive/MyDrive/fasttext.bin',max_vocab_size=5000000)"],"metadata":{"id":"P0uChI0-KWXZ"},"id":"P0uChI0-KWXZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_model.wv.most_similar('한글')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aP1zit0Q_If0","executionInfo":{"status":"ok","timestamp":1674993138833,"user_tz":-540,"elapsed":6,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"11590cf3-0bf4-45d6-fb9d-3de49c072369"},"id":"aP1zit0Q_If0","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('한글모', 0.829578697681427),\n"," ('훈민정음', 0.7940126061439514),\n"," ('아래아', 0.7832252979278564),\n"," ('맞춤법', 0.7825278043746948),\n"," ('한중사전', 0.7771036624908447),\n"," ('훈민정음해례', 0.7737016677856445),\n"," ('아래아한글', 0.758316695690155),\n"," ('안마태', 0.7563860416412354),\n"," ('한자사전', 0.7555388808250427),\n"," ('훈몽자회', 0.7461692094802856)]"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","id":"c7ed2fa1","metadata":{"id":"c7ed2fa1"},"source":["# 책에서 다루는 신경망 모델\n","1. 빈도 기반 DTM 행렬로 MLP (07-11)\n","2. 임베딩 레이어 없이 1D 신경망 (08-04)\n","3. 임베딩 레이어 다음에 GlobalAveragePooling1D() 추가해서 평균 벡터로 1D 신경망(09-12)\n","4. 임베딩 레이어 없이 단어벡터 평균내서 1D 신경망 (09-11)\n","4. RNN, CNN, LSTM, BiLSTM (10~)\n","\n","# 자연어 처리 딥러닝 모델 구현\n","자연어 처리는 사전학습된 임베딩 모델을 불러와 내 데이터에 맞추는것이 효율적이다.  \n","그렇다면 임베딩 벡터값을 그대로 가져오는것보다 내 데이터에 맞는 값을 가져오는게 더 좋지 않을까?  \n","이를 비교하기 위해 파인튜닝 유무(_맨 아래 임베딩층까지 학습을 시키냐 안시키냐_) 에 따른 성능비교를 하고자 한다  \n","\n","![image](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-26_at_11.06.50_PM.png)  \n","\n","[파인튜닝에 대해 잘 정리해둔 블로그](https://inhovation97.tistory.com/31)  \n","\n","***\n","헷갈리는거\n","[RNN 구조 설명](https://sonsnotation.blogspot.com/2020/11/10-recurrent-neural-networkrnn.html)<br>\n","1. RNN은 시퀀스(순차 데이터, 순서가 있는 항목의 모음)을 처리하는 모델. 즉 input이 여러개임\n","2. 임베딩 레이어 없으면 직접 (배치,문장 내 단어,임베딩 차원수)로 구성된 텐서를 input에 넣어야함\n","\n","## 파인튜닝 유무 비교"]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from keras.layers import Embedding"],"metadata":{"id":"juUTIK11zfZW","executionInfo":{"status":"ok","timestamp":1674993074592,"user_tz":-540,"elapsed":4508,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3025dcd-430a-42d2-a099-d0dcf375e2bd"},"id":"juUTIK11zfZW","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import warnings \n","warnings.filterwarnings(action='ignore')\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import re"],"metadata":{"id":"jFVLF2mUbtDN"},"id":"jFVLF2mUbtDN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('/content/drive/MyDrive/NLP/NLP_study/nlp_data/practice.csv',index_col=0)\n","test = pd.read_csv('/content/drive/MyDrive/NLP/NLP_study/nlp_data/test.csv',index_col=0)"],"metadata":{"id":"CTq770TNb3mJ"},"id":"CTq770TNb3mJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 전처리는 데이콘 베이스라인으로 진행"],"metadata":{"id":"rR48FhHYeKea"},"id":"rR48FhHYeKea"},{"cell_type":"code","source":["#부호를 제거해주는 함수\n","def alpha_num(text):\n","    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n","\n","train['text']=train['text'].apply(alpha_num)\n","\n","# 불용어 제거해주는 함수\n","def remove_stopwords(text):\n","    final_text = []\n","    for i in text.split():\n","        if i.strip().lower() not in stopwords:\n","            final_text.append(i.strip())\n","    return \" \".join(final_text)\n","\n","# 불용어\n","stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n","             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n","             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n","             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n","             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n","             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n","             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n","             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n","             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n","             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n","             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n","\n","#전처리 적용\n","train['text'] = train['text'].str.lower()\n","test['text'] = test['text'].str.lower()\n","train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords)\n","test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords)\n","\n","# train test 분리\n","X_train = np.array([x for x in train['text']])\n","X_test = np.array([x for x in test['text']])\n","y_train = np.array([x for x in train['score']])"],"metadata":{"id":"fFDWxEdAb3gn"},"id":"fFDWxEdAb3gn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#파라미터 설정\n","vocab_size = 5000\n","embedding_dim = 300\n","max_length = 100\n","padding_type='post'\n","#oov_tok = \"<OOV>\"\n","\n","#tokenizer에 fit\n","tokenizer = Tokenizer(num_words = vocab_size)#, oov_token=oov_tok)\n","tokenizer.fit_on_texts(X_train)\n","word_index = tokenizer.word_index\n","\n","#데이터를 sequence로 변환해주고 padding 해줍니다.\n","train_sequences = tokenizer.texts_to_sequences(X_train)\n","train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n","\n","test_sequences = tokenizer.texts_to_sequences(X_test)\n","test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)"],"metadata":{"id":"ydkMEPd4dTMP"},"id":"ydkMEPd4dTMP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 사전학습 모델 불러오기"],"metadata":{"id":"_NOsIoqPeGJn"},"id":"_NOsIoqPeGJn"},{"cell_type":"code","source":["import gensim\n","word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin', \n","                                                                 binary=True,\n","                                                                 limit=1000000)\n","\n","print('모델의 크기(shape) :',word2vec_model.vectors.shape) # 모델의 크기 확인"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIujKFpHdwqq","executionInfo":{"status":"ok","timestamp":1674996067176,"user_tz":-540,"elapsed":29381,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"8febb90b-d896-4a47-82e5-1b226185a7a7"},"id":"OIujKFpHdwqq","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["모델의 크기(shape) : (1000000, 300)\n"]}]},{"cell_type":"code","source":["embedding_matrix = np.zeros((vocab_size, 300))"],"metadata":{"id":"PHxHiet8e0UV"},"id":"PHxHiet8e0UV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_vector(word):\n","    if word in word2vec_model:\n","        return word2vec_model[word]\n","    else:\n","        return None"],"metadata":{"id":"bGtcNAX3fE7g"},"id":"bGtcNAX3fE7g","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for word, index in tokenizer.word_index.items():\n","    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n","    vector_value = get_vector(word)\n","    if vector_value is not None:\n","        embedding_matrix[index] = vector_value"],"metadata":{"id":"QEH07jM2fE4h"},"id":"QEH07jM2fE4h","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('단어 nice의 맵핑된 정수 :', tokenizer.word_index['nice'])\n","print(embedding_matrix[1462]==word2vec_model['nice'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"va_YbUipfE0E","executionInfo":{"status":"ok","timestamp":1674996611859,"user_tz":-540,"elapsed":4,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"d1d736b8-1fdb-4e62-fe94-c88ca87c8172"},"id":"va_YbUipfE0E","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 nice의 맵핑된 정수 : 1462\n","[ True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True]\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.layers import Flatten, Input\n","\n","model = Sequential()\n","model.add(Input(shape=(max_length,), dtype='int32'))\n","e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False)\n","model.add(e)\n","model.add(Dense(64, activation='relu'))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(5, activation='softmax'))\n","\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","\n","# model summary\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A1uNiHvbfEvD","executionInfo":{"status":"ok","timestamp":1674996672254,"user_tz":-540,"elapsed":417,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"fb91293a-c7e7-45e9-cc75-3176e2d50faf"},"id":"A1uNiHvbfEvD","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_6 (Embedding)     (None, 100, 300)          1500000   \n","                                                                 \n"," dense_17 (Dense)            (None, 100, 64)           19264     \n","                                                                 \n"," flatten_5 (Flatten)         (None, 6400)              0         \n","                                                                 \n"," dense_18 (Dense)            (None, 32)                204832    \n","                                                                 \n"," dense_19 (Dense)            (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 1,724,129\n","Trainable params: 224,129\n","Non-trainable params: 1,500,000\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["# fit model\n","num_epochs = 5\n","history = model.fit(train_padded, y_train, \n","                    epochs=num_epochs, verbose=2, \n","                    validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PURJ77jzb3Z-","executionInfo":{"status":"ok","timestamp":1674996757756,"user_tz":-540,"elapsed":82705,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"4d223742-234a-4d6e-d43d-eadf4be41704"},"id":"PURJ77jzb3Z-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1372/1372 - 17s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 17s/epoch - 12ms/step\n","Epoch 2/5\n","1372/1372 - 16s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 16s/epoch - 11ms/step\n","Epoch 3/5\n","1372/1372 - 16s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 16s/epoch - 12ms/step\n","Epoch 4/5\n","1372/1372 - 16s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 16s/epoch - 12ms/step\n","Epoch 5/5\n","1372/1372 - 16s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 16s/epoch - 12ms/step\n"]}]},{"cell_type":"code","source":["#파인튜닝\n","model = Sequential()\n","model.add(Input(shape=(max_length,), dtype='int32'))\n","e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=True)\n","model.add(e)\n","model.add(Dense(64, activation='relu'))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(5, activation='softmax'))\n","\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","\n","# model summary\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KrTzM9TVlkfW","executionInfo":{"status":"ok","timestamp":1674996794065,"user_tz":-540,"elapsed":366,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"74c05d66-3d20-4f89-8ad1-3073f1723eca"},"id":"KrTzM9TVlkfW","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_7 (Embedding)     (None, 100, 300)          1500000   \n","                                                                 \n"," dense_20 (Dense)            (None, 100, 64)           19264     \n","                                                                 \n"," flatten_6 (Flatten)         (None, 6400)              0         \n","                                                                 \n"," dense_21 (Dense)            (None, 32)                204832    \n","                                                                 \n"," dense_22 (Dense)            (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 1,724,129\n","Trainable params: 1,724,129\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","source":["대충 만든 모델이라 학습이 되지 않음을 알 수 있다. 사실 이러면 파인튜닝하는 의미가 없다"],"metadata":{"id":"ZMBtprfclyCy"},"id":"ZMBtprfclyCy"},{"cell_type":"code","source":["# fit model\n","num_epochs = 5\n","history = model.fit(train_padded, y_train, \n","                    epochs=num_epochs, verbose=2, \n","                    validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nj74hBpWlsXb","executionInfo":{"status":"ok","timestamp":1674997002441,"user_tz":-540,"elapsed":196356,"user":{"displayName":"김평진","userId":"11705489940506901507"}},"outputId":"14508bdd-10f9-4190-b19d-9d3e628a8c86"},"id":"Nj74hBpWlsXb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1372/1372 - 46s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 46s/epoch - 33ms/step\n","Epoch 2/5\n","1372/1372 - 39s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 39s/epoch - 28ms/step\n","Epoch 3/5\n","1372/1372 - 37s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 37s/epoch - 27ms/step\n","Epoch 4/5\n","1372/1372 - 38s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 38s/epoch - 28ms/step\n","Epoch 5/5\n","1372/1372 - 37s - loss: 0.0000e+00 - acc: 0.1310 - val_loss: 0.0000e+00 - val_acc: 0.1340 - 37s/epoch - 27ms/step\n"]}]},{"cell_type":"markdown","source":["사전학습 vs 그냥 학습\n","[잘 정리된 링크](https://towardsdatascience.com/a-guide-to-word-embeddings-8a23817ab60f) \n","\n","일반적인 용어면 사전학습이 좋지만, 전문용어나 산업용어라면 학습시키는게 더 낫다. 물론 둘 다 해보고 정해라"],"metadata":{"id":"Qkx0UeCyrseI"},"id":"Qkx0UeCyrseI"},{"cell_type":"markdown","source":["![RNN DNN](https://lh3.googleusercontent.com/-nJ9Du8mwBLA/X7dQdwFpAzI/AAAAAAAAOeU/dFhTrtWe_Wc2HkbOdtENjHxr54OMjQzPwCLcBGAsYHQ/w534-h211/image.png)  \n","\n"],"metadata":{"id":"JCNKsIJusBub"},"id":"JCNKsIJusBub"},{"cell_type":"markdown","source":["[파인튜닝이 항상 좋진 않음](https://dev.to/meetkern/how-to-fine-tune-your-embeddings-for-better-similarity-search-445e)\n","\n","[재학습과 파인튜닝](https://www.quora.com/What-is-the-difference-between-fine-tuning-and-retraining-for-the-pre-trained-word-embedding-model-such-as-Glove)"],"metadata":{"id":"T8nBwmoHm-E7"},"id":"T8nBwmoHm-E7"},{"cell_type":"code","source":[],"metadata":{"id":"xWOZ16QVndEK"},"id":"xWOZ16QVndEK","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"provenance":[],"collapsed_sections":["wXSe0TbjSzOE"]}},"nbformat":4,"nbformat_minor":5}